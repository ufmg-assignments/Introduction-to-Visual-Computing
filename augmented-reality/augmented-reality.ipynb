{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho Prático 2: Realidade Aumentada\n",
    "\n",
    " \n",
    "Nome: Arthur Pontes Nader\\\n",
    "Matrícula: 2019022294\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Trabalho Prático 2 da disciplina Introdução a Computação Visual consistiu na detecção e localização de alvos nos frames de um vídeo e na inserção de objetos tridimensionais sobre esses alvos. Os seguintes links correspondem a vídeos no YouTube com a explicação do trabalho e mostrando os resultados obtidos:\n",
    "\n",
    "Explicação do trabalho: https://www.youtube.com/watch?v=ok7CfWbfWew\n",
    "\n",
    "\n",
    "Detecção dos alvos: https://www.youtube.com/watch?v=CLn6HYKVIDo\n",
    "\n",
    "\n",
    "Realidade aumentada: https://www.youtube.com/watch?v=RH9U3x8m8P0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realização do trabalho, utilizou-se as seguintes bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pygame\n",
    "\n",
    "from OpenGL.GL import *\n",
    "from OpenGL.GLUT import *\n",
    "from OpenGL.GLU import *\n",
    "\n",
    "from objloader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Calibração da câmera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar a calibração da câmera, utilizou-se a ferramenta Octave 5.2.0 e o toolbox de Jean-Yves-Bouguet disponibilizada na seguinte página: https://github.com/nghiaho12/camera_calibration_toolbox_octave. \n",
    "\n",
    "Usou-se 7 frames do vídeo que forneciam uma medida da cena a partir de diferentes posições e orientações da câmera. Os resultados podem ser visualizados a seguir, sendo que ao final do notebook há um apêndice com os resultados oficiais fornecidos pelo Octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "centro_otico_x = 306.31489\n",
    "centro_otico_y = 235.42074 \n",
    "distancia_focal_x = 413.19647\n",
    "distancia_focal_y = 412.39518\n",
    "skew = 0.00\n",
    "coeficientes_distorcao = np.float64([[0.08027, -0.19506, -0.00007, -0.00781, 0.00000]])\n",
    "pixel_erro = [0.16675, 0.20123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com esses valores, pode-se gerar a seguinte matriz de parâmetros intrínsecos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_intrinsecos = np.float64([[distancia_focal_x, skew, centro_otico_x],\n",
    "                                [0, distancia_focal_y, centro_otico_y],\n",
    "                                [0,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Detecção dos alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função gerarAlvos recebe como parâmetro o alvo e, após convertê-lo para uma imagem binária, o rotaciona diversas vezes para que posteriormente seja possível comparar se uma imagem corresponde ao alvo em uma orientação diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerarAlvos(img):\n",
    "    \n",
    "    alvo_cinza = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    alvo_binario = cv2.threshold(alvo_cinza, 108, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    alvo1 = alvo_binario\n",
    "    alvo2 = np.rot90(alvo1)\n",
    "    alvo3 = np.rot90(alvo2)\n",
    "    alvo4 = np.rot90(alvo3)\n",
    "    \n",
    "    return alvo1, alvo4, alvo3, alvo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função detectarQuadrilateros recebe um frame do video e, após convertê-lo para uma imagem binária, detecta os contornos presentes. A seguir, uma série de filtragens, como relacionada a área e o número de vértices do contorno, são realizadas para que somente se tenha os resultados mais relevantes. Como se está interessado em idenficar um quadrilátero, só serão retornados os contornos que possuírem 4 vértices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectarQuadrilateros(frame):\n",
    "\n",
    "    img_cinza = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    ret, image_bin = cv2.threshold(img_cinza, 127, 255, 0)\n",
    "\n",
    "    deteccoes, hierarquia = cv2.findContours(image_bin, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    deteccoes = [det for det in deteccoes if cv2.contourArea(det) < 20000.0 and cv2.contourArea(det) > 1500.0]\n",
    "    deteccoes = sorted(deteccoes, key = cv2.contourArea, reverse = True)[:5]\n",
    "    \n",
    "    quadrilateros = []\n",
    "    for det in deteccoes:\n",
    "        quad = cv2.approxPolyDP(det, 0.02 * cv2.arcLength(det, True), True)\n",
    "        if len(quad) == 4:\n",
    "            quadrilateros.append(quad)\n",
    "    \n",
    "    return quadrilateros\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já a função detectarAlvos recebe os quadriláteros detectados no frame e, por meio da realização de uma homografia, retifica e gera uma nova imagem com a porção do frame correspondente ao quadrilátero. Assim, pode-se usar um algoritmo de Template Matching para confirmação se o quadrilátero realmente é um alvo, e caso seja, qual é a sua orientação. \n",
    "Os seguintes métodos podem ser passados como parâmetro na chamada da função para realizar o Template Matching: \n",
    "* TM_CCORR_NORMED\n",
    "* TM_CCOEFF_NORMED\n",
    "* TM_SQDIFF_NORMED\n",
    "\n",
    "Por padrão, utilizou-se o método TM_CCORR_NORMED, pois esse método apresentou resultados satisfatórios na detecção do alvo.\n",
    "\n",
    "Para diferenciar cada um dos alvos na imagem, utilizou-se uma manipulação direta dos pixels, já que se pecebeu que o alvo 1 só aparecia na metade esquerda e o alvo 2 só ocorria na parte superior do video. Assim, o alvo 1 será contornado de verde, o 2 de azul e o 3 de amarelo. Essa diferenciação será usada posteriormente para determinar o sentido de rotação do Pikachu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectarAlvo(x, y, quadrilateros, frame, alvos, metodo = cv2.TM_CCORR_NORMED, limite = 0.75):\n",
    "    \n",
    "    alvos_localizados = []\n",
    "    \n",
    "    for quad in quadrilateros:\n",
    "        \n",
    "        coordenadas = [[0,0],[x, 0],[x,y],[0, y]]\n",
    "        matriz_homografia, _ = cv2.findHomography(np.float64(quad), np.float64(coordenadas))\n",
    "        alvo_retificado = cv2.warpPerspective(frame, matriz_homografia, (x,y))\n",
    "        \n",
    "        img_cinza = cv2.cvtColor(alvo_retificado, cv2.COLOR_BGR2GRAY)\n",
    "        ret, image_bin = cv2.threshold(img_cinza, 127, 255, 0)\n",
    "        \n",
    "        coords_x = [quad[k][0][0] < frame.shape[1]//2 for k in range(4)]\n",
    "        coords_y = [quad[k][0][1] < frame.shape[0]//2 for k in range(4)]\n",
    "        \n",
    "        alvo_1 = coords_x[0] and coords_x[1] and coords_x[2] and coords_x[3]\n",
    "        alvo_2 = coords_y[0] and coords_y[1] and coords_y[2] and coords_y[3]\n",
    "            \n",
    "        \n",
    "        for j in range(len(alvos)):\n",
    "            resultado = cv2.matchTemplate(image_bin, alvos[j], metodo)\n",
    "            \n",
    "            if metodo == cv2.TM_SQDIFF_NORMED:\n",
    "                resultado = 1.0 - resultado\n",
    "            \n",
    "            if resultado > limite:\n",
    "                if alvo_1:\n",
    "                    cv2.drawContours(frame, [quad], -1, (55,200, 20), 2)\n",
    "                    alvos_localizados.append((quad,\"alvo1\",j))\n",
    "                elif alvo_2:\n",
    "                    cv2.drawContours(frame, [quad], -1, (200,55, 20), 2)\n",
    "                    alvos_localizados.append((quad,\"alvo2\",j))\n",
    "                else:\n",
    "                    cv2.drawContours(frame, [quad], -1, (55,200, 200), 2)\n",
    "                    alvos_localizados.append((quad,\"alvo3\",j))\n",
    "                \n",
    "                proximo_vertice = (j+1)%4\n",
    "                x_centro = int((quad[0][0][0]+quad[2][0][0])/2)\n",
    "                y_centro = int((quad[0][0][1]+quad[2][0][1])/2)\n",
    "                x_frente = int((quad[j][0][0]+quad[proximo_vertice][0][0])/2)\n",
    "                y_frente = int((quad[j][0][1]+quad[proximo_vertice][0][1])/2)\n",
    "                cv2.arrowedLine(frame, (x_centro, y_centro), (x_frente, y_frente), (20,55,200), 2)\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return frame, alvos_localizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Obtenção da pose da câmera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função encontrarPose recebe a matriz de parâmetros intrinsecos, os coeficientes de distorção, a localização do alvo na cena e a coordenadas de seus vértices no mundo. Assim por meio da utilização do método solvePnP, gera-se um vetor de rotação e um vetor de translação, que, após algumas manipulações, são transformados na matriz de parâmetros extrínsecos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrarPose(intrinsecos, mundo, distorcao, alvo):\n",
    "\n",
    "    alvo_aux = np.float64([ele[0] for ele in alvo])\n",
    "    \n",
    "    ret, vetor_rotacao, vetor_translacao = cv2.solvePnP(mundo, alvo_aux, intrinsecos, distorcao)\n",
    "    \n",
    "    matriz_extrinsecos, _ = cv2.Rodrigues(vetor_rotacao)\n",
    "    matriz_extrinsecos = np.concatenate([matriz_extrinsecos, vetor_translacao], axis=1)\n",
    "    matriz_extrinsecos = np.concatenate((matriz_extrinsecos, [[0.0,0.0,0.0,1.0]]), axis=0)\n",
    "    \n",
    "    return matriz_extrinsecos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao final desse notebook, há um código que mostra os resultados de tudo que foi feito até essa parte do trabalho. Nesse código, a função encontrarPose é um pouco diferente para que se possa observar a projeção de um ponto na cena utilizando a matriz gerada pela função encontrarPose. (vale a pena conferir, é bem legal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Renderização do cubo e do Pikachu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função object3D a seguir recebe como parâmetros o objeto a ser renderizado, a matriz gerada em encontrarPose e o número do alvo em questão. \n",
    "Primeiramente, a matriz é transformada para se adequar ao sistema de coordenadas da OpenGL. Então, renderiza-se um cubo no local e uma seta que indica a frente do alvo. \\\n",
    "Após isso, o Pikachu é renderizado no mesmo local. Só que, se o alvo identificado for o número 1 ou o 3, o Pikachu será rotacionado no sentido anti-horário. Caso seja o 2, ele é rotacionado no sentido horário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object3D(obj, matriz, num_alvo):\n",
    "    \n",
    "    matriz[1,:] = -matriz[1,:]\n",
    "    matriz[2,:] = -matriz[2,:]\n",
    "    matriz = matriz.T\n",
    "    matriz = matriz.flatten() \n",
    "    \n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glLoadIdentity()\n",
    "    glLoadMatrixf(matriz)\n",
    "    glLineWidth(2.15)\n",
    "    glBegin(GL_LINES)\n",
    "    glVertex3f(2.5,2.5,0)\n",
    "    glVertex3f(2.5,10.5,0)\n",
    "    glVertex3f(2.5,10.5,0)\n",
    "    glVertex3f(1.7,8.7,0)\n",
    "    glVertex3f(2.5,10.5,0)\n",
    "    glVertex3f(3.3,8.7,0)\n",
    "    glEnd()\n",
    "    glutWireCube(7.8)\n",
    "    \n",
    "    global rotacao\n",
    "    glLoadIdentity()\n",
    "    glLoadMatrixf(matriz)\n",
    "    \n",
    "    if num_alvo == \"alvo1\" or num_alvo == \"alvo3\":\n",
    "        glRotate(rotacao, 0, 0, 1)\n",
    "    elif num_alvo == \"alvo2\":\n",
    "        glRotate(-rotacao, 0, 0, 1)\n",
    "        \n",
    "    glScalef(2.31, 2.31, 2.31)\n",
    "    glCallList(obj.gl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Funções da OpenGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções a seguir são usadas para inicializar, definir, habilitar e limpar alguns paramêtros de execução da OpenGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initOpenGL(dimensions):\n",
    "\n",
    "    (width, height) = dimensions\n",
    "    \n",
    "    glClearColor(0.0, 0.0, 0.0, 0.0)\n",
    "    glClearDepth(1.0)\n",
    "\n",
    "    glEnable(GL_DEPTH_TEST)\n",
    "\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glLoadIdentity()\n",
    " \n",
    "    fovy = 2*np.arctan(0.5*height/distancia_focal_y)*180/np.pi;\n",
    "    aspect = (width*distancia_focal_x)/(height*distancia_focal_y);\n",
    "    gluPerspective(fovy, aspect, 0.1, 100.0)\n",
    "    \n",
    "def idleCallback():\n",
    "    \n",
    "    glutPostRedisplay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já a função aplicarFundo recebe um frame do video e, após criar uma identificação para a textura, realiza as operações que irão mapear o frame como fundo da cena em que os objetos serão renderizados. Em seguida a textura criada é deletada. (reparou-se que, se isso não for feito, parece que há um vazamento de memória, fazendo com que a exibição dos resultados na tela fique cada vez mais lenta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicarFundo(fundo, largura, altura):\n",
    "    \n",
    "    fundo = cv2.flip(fundo, 0)\n",
    "    fundo = cv2.cvtColor(fundo, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    nome_textura = glGenTextures(1)\n",
    "    \n",
    "    glBindTexture(GL_TEXTURE_2D, nome_textura)\n",
    "    glTexImage2D(GL_TEXTURE_2D,0,GL_RGB,largura,altura,0,GL_RGB, GL_UNSIGNED_BYTE, fundo)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER,GL_NEAREST)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER,GL_NEAREST)\n",
    "    \n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPushMatrix()\n",
    "    glLoadIdentity()\n",
    "    gluOrtho2D(-1, 1, -1, 1)\n",
    "    \n",
    "    glBegin(GL_QUADS)\n",
    "    glTexCoord2f(0.0, 0.0); glVertex3f(-1.0, -1.0, 0.0)\n",
    "    glTexCoord2f(1.0, 0.0); glVertex3f(1.0, -1.0, 0.0)\n",
    "    glTexCoord2f(1.0, 1.0); glVertex3f(1.0, 1.0, 0.0)\n",
    "    glTexCoord2f(0.0, 1.0); glVertex3f(-1.0, 1.0, 0.0) \n",
    "    glEnd()\n",
    "    glPopMatrix()\n",
    "    \n",
    "    glDeleteTextures(1, nome_textura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função displayCallback é acionada continuamente por glutMainLoop. Então, será nela que deverão ser chamados os procedimentos de carregamento do modelo 3D do Pikachu, leitura de frame do vídeo, aplicação do frame como fundo da cena, detecção dos alvos, cálculo da pose e renderização dos objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayCallback():\n",
    "    \n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glLoadIdentity()\n",
    "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "    \n",
    "    # carregar o modelo 3D do Pikachu\n",
    "    obj = OBJ(\"Pikachu.obj\", swapyz=True)\n",
    "    \n",
    "    # habilita o uso de texturas (o Pikachu tem textura)\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "    \n",
    "    global captura\n",
    "    ret, frame = captura.read()\n",
    "    \n",
    "    if ret:    \n",
    "        aplicarFundo(frame, frame.shape[1], frame.shape[0])\n",
    "        quads = detectarQuadrilateros(frame)\n",
    "        _, alvos_localizados = detectarAlvo(alvo.shape[0], alvo.shape[1], quads, frame, alvos)\n",
    "    \n",
    "        for alvo_loc in alvos_localizados:\n",
    "            mundo_aux = coordenadas_mundo.copy()\n",
    "            for i in range(alvo_loc[2]):\n",
    "                mundo_aux = np.concatenate(([mundo_aux[3].copy()],mundo_aux), axis=0)\n",
    "                mundo_aux = np.delete(mundo_aux, 4, 0)\n",
    "\n",
    "            pose = encontrarPose(matriz_intrinsecos, mundo_aux, coeficientes_distorcao, alvo_loc[0])\n",
    "            object3D(obj, pose, alvo_loc[1])\n",
    "        \n",
    "        global rotacao\n",
    "        rotacao += 5\n",
    "        rotacao = rotacao%360\n",
    "            \n",
    "    glutSwapBuffers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Execução do programa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, com todas as funções definidas, as variáveis a seguir são declaradas para serem utilizadas ao longo da execução. Elas se referem ao carregamento e geração dos alvos, ao sistemas de coordenadas no mundo correspondente a um alvo detectado, à rotação do Pikachu e ao carregamento do vídeo de entrada. Em seguida, inicia-se a execução do programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo = cv2.imread(\"alvo.jpg\")\n",
    "alvos = gerarAlvos(alvo)\n",
    "coordenadas_mundo = np.float64([[0,5,0],[5,5,0],[5,0,0],[0,0,0]])\n",
    "rotacao = 0\n",
    "captura = cv2.VideoCapture('entrada.mp4')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dimensions = (640, 480)\n",
    "    glutInit()\n",
    "    glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE)\n",
    "    glutSetOption(GLUT_ACTION_ON_WINDOW_CLOSE, GLUT_ACTION_CONTINUE_EXECUTION)\n",
    "    glutInitWindowSize(*dimensions)\n",
    "    window = glutCreateWindow(b'TP2 - Realidade Aumentada')\n",
    "    \n",
    "    initOpenGL(dimensions)\n",
    "    \n",
    "    glutDisplayFunc(displayCallback)\n",
    "    glutIdleFunc(idleCallback)\n",
    "    \n",
    "    glutMainLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A realização desse trabalho prático foi uma boa oportunidade de colocar em prática os conceitos aprendidos durante a segunda parte da disciplina Introdução a Computação Visual. A sua implementação possibitou fixar vários dos métodos de computação visual vistos durante as aulas, como homografia, template matching, estimação de pose, renderização de objetos e mapeamento de textura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Referências Bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Livros\n",
    "* SOLEM, J, E. Programming Computer Vision with Python, O'Reilly, 2012. 300 p.\n",
    "* WRIGHT, R, S. Jr.; SWEET, M. OpenGL SuperBible. 2nd ed. Indianapolis, Indiana: Waite Group Press, 2000. 696 p.\n",
    "\n",
    "### Sites\n",
    "\n",
    "##### Calibração da câmera\n",
    "* https://learnopencv.com/camera-calibration-using-opencv/\n",
    "* https://github.com/nghiaho12/camera_calibration_toolbox_octave\n",
    "\n",
    "##### Homografia\n",
    "* https://learnopencv.com/homography-examples-using-opencv-python-c/\n",
    "\n",
    "##### Detecção do alvo\n",
    "* https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n",
    "* https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html\n",
    "* https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac\n",
    "\n",
    "##### Numpy\n",
    "* https://numpy.org/doc/stable/reference/constants.html\n",
    "* https://numpy.org/doc/stable/reference/generated/numpy.arctan.html\n",
    "\n",
    "##### OpenGL\n",
    "* https://www.inf.pucrs.br/~manssour/OpenGL/Introducao.html\n",
    "* http://pyopengl.sourceforge.net/documentation/manual-3.0/glDeleteTextures.html\n",
    "* https://www.inf.pucrs.br/~pinho/CG/Aulas/OpenGL/Texturas/MapTextures.html\n",
    "* https://itecnote.com/tecnote/opencv-reference-coordinate-system-changes-between-opencv-opengl-and-android-sensor/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Apêndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados da calibração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration results after optimization (with uncertainties):\n",
    "\n",
    "Focal Length:          fc = [ 413.19647   412.39518 ] +/- [ 10.93237   9.94818 ]\\\n",
    "Principal point:       cc = [ 306.31489   235.42074 ] +/- [ 4.84831   8.18127 ]\\\n",
    "Skew:             alpha_c = [ 0.00000 ] +/- [ 0.00000  ]   => angle of pixel axes = 90.00000 +/- 0.0000\n",
    "0 degrees\\\n",
    "Distortion:            kc = [ 0.08027   -0.19506   -0.00007   -0.00781  0.00000 ] +/- [ 0.05616   0.389\n",
    "20   0.00584   0.00498  0.00000 ]\\\n",
    "Pixel error:          err = [ 0.16675   0.20123 ]\n",
    "\n",
    "Note: The numerical errors are approximately three times the standard deviations (for reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados da identificação do alvo na cena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo = cv2.imread(\"alvo.jpg\")\n",
    "alvos = gerarAlvos(alvo)\n",
    "\n",
    "coordenadas_mundo = np.float64([[0,5,0],[5,5,0],[5,0,0],[0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrarPose2(intrinsecos, mundo, distorcao, frame, alvo):\n",
    "\n",
    "    alvo_aux = np.float64([ele[0] for ele in alvo])\n",
    "    \n",
    "    ret, vetor_rotacao, vetor_translacao = cv2.solvePnP(mundo, alvo_aux, intrinsecos, distorcao)\n",
    "    \n",
    "    matriz_extrinsecos, _ = cv2.Rodrigues(vetor_rotacao)\n",
    "    destino, _ = cv2.projectPoints(np.array([(2.5, 2.5, 5.0)]), vetor_rotacao, vetor_translacao,intrinsecos, distorcao)\n",
    "    \n",
    "    for ponto in alvo_aux:\n",
    "        cv2.circle(frame, (int(ponto[0]), int(ponto[1])), 3, (0,0,255), -1)\n",
    "    \n",
    "    p1 = int((alvo[0][0][0]+alvo[2][0][0])/2), int((alvo[0][0][1]+alvo[2][0][1])/2)\n",
    "    p2 = int(destino[0][0][0]), int(destino[0][0][1])\n",
    "    cv2.line(frame, p1, p2, (255,100,60), 2)\n",
    "    \n",
    "    matriz_extrinsecos = np.concatenate([matriz_extrinsecos, vetor_translacao], axis=1)\n",
    "    matriz_extrinsecos = np.concatenate((matriz_extrinsecos, [[0.0,0.0,0.0,1.0]]), axis=0)\n",
    "    \n",
    "    return frame, matriz_extrinsecos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "captura = cv2.VideoCapture('entrada.mp4')\n",
    "\n",
    "while(captura.isOpened()):\n",
    "    ret, frame = captura.read()\n",
    "    if ret == True:\n",
    "\n",
    "        quads = detectarQuadrilateros(frame)\n",
    "        deteccao, alvos_localizados = detectarAlvo(alvo.shape[0], alvo.shape[1], quads, frame, alvos)\n",
    "\n",
    "        for alvo_loc in alvos_localizados:\n",
    "            mundo_aux = coordenadas_mundo.copy()\n",
    "            for i in range(alvo_loc[2]):\n",
    "                mundo_aux = np.concatenate(([mundo_aux[3].copy()],mundo_aux), axis=0)\n",
    "                mundo_aux = np.delete(mundo_aux, 4, 0)\n",
    "\n",
    "            deteccao, _ = encontrarPose2(matriz_intrinsecos, mundo_aux, coeficientes_distorcao, deteccao, alvo_loc[0])\n",
    "            \n",
    "        cv2.imshow(\"Deteccao dos alvos\", deteccao)\n",
    "        \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "captura.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
